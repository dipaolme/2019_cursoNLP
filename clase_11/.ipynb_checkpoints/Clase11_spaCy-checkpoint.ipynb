{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase 11\n",
    "\n",
    "# <center>spaCy</center>\n",
    "\n",
    "---\n",
    "\n",
    "[spaCy](https://spacy.io/) es una librería open-source diseñada para realizar tareas de NLP.\n",
    "Su código, disponible en [GitHub](https://github.com/explosion/spaCy), se encuentra escrito en Python y Cython. Este último permite enriquecer la escritura de Python con propiedades de los lenguajes C y C++, haciéndolo más eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guía de clase:**\n",
    "\n",
    "- [1. Importar la librería](#1.-Importar-la-librería)\n",
    "- [2. Configurar un pipeline](#2.-Configurar-un-pipeline)\n",
    "- [3. Doc](#3.-Doc)\n",
    "- [4. Tokenización](#4.-Tokenización)\n",
    "- [5. Etiquetado](#5.-Etiquetado)\n",
    "- [6. Parseo de dependencias](#6.-Parseo-de-dependencias)\n",
    "- [7. Segmentación de oraciones](#7.-Segmentación-de-oraciones)\n",
    "- [8. NER](#8.-NER)\n",
    "- [9. Matcheo-mediante-reglas](#9.-Matcheo-mediante-reglas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar la librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6f70af4282c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy cuenta con una serie de modelos pre-entrenados para distintas lenguas (inglés, alemán, español, francés, portugués, italiano, holandés y griego). Para poder usarlos, es necesarios instalarlos luego de instalar la librería, del mismo modo que instalamos cualquier paquete de Python.\n",
    "\n",
    "Asimismo, una vez importada la librería, debe instanciarse con el entrenamiento deseado. De él tomará los distintos componentes para los análisis lingüísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos un entrenamiento en español\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualmente, SpaCy cuenta con dos modelos pre-entrenados para el español:\n",
    "\n",
    "- es_core_news_sm: modelo multipropósito entrenado con textos de medios y redes sociales. Contiene componentes para realizar tokenización, POS tagging, parsers de dependencia y reconocer entidades (PER, LOC, ORG, MISC)\n",
    "\n",
    "- es_core_news_md: modelo multipropósito que, además de contener los componentes del modelo anterior, presenta un tamaño mayor e incluye vectores de palabras\n",
    "\n",
    "Ambos modelos toman como fuente textos extraídos de AnCora, Wikipedia y WikiNER Corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurar un pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./pictures/pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El atributo *pipe_names* nos permite acceder a los nombre de los componentes que se encuentran incluidos en el modelo. Estos se muestran en orden de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no quisiéramos utilizar todos los componentes incluidos (ya sea porque no los necesitamos o porque queremos cambiar el componente por defecto por uno hecho por nosotros mismos), podemos deshabilitar aquellos que queramos excluir utilizando al momento de cargar el idioma, o bien deshabilitándolos luego con el método *disable_pipes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_parser = spacy.load(\"es\", disable=[\"ner\", \"tagger\"])\n",
    "\n",
    "nlp_parser.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disabled = nlp.disable_pipes('tagger', 'parser')\n",
    "nlp.begin_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también podemos restaurar los valores por default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disabled.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy ya viene con información lingüística y anotaciones que podemos utilizar para enriquecer nuestros textos crudos. Para ello, debemos construir un objeto ```Doc```, el cual consite en una secuencia de objetos ```Token``` con distintas anotaciones según cuál sea el modelo pre-entrenado que estemos utilizando y cómo hayamos configurado nuestro pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bestiario/casa_tomada.txt','r') as f:\n",
    "    casa_tomada = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(casa_tomada[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(casa_tomada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad de tokens en el texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:20]:\n",
    "    print(token.i, token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizador de SpaCy:\n",
    "\n",
    "1. El texto crudo se segmenta por espacios, como cuando usamos text.split(' ')\n",
    "2. El tokenizador recorre el texto de izquierda a derecha y, para cada substring o posible palabra encontrada, realiza dos verificaciones:\n",
    "\n",
    "    2.1. Evalúa si la posible palabra forma parte de alguna excepción y que entonces deba seguir alguna regla particular\n",
    "    \n",
    "    2.2 Busca signos especiales como comas, paréntesis o comillas que puedan ser abiertos, cerrados o estar entre las palabras y deban separarse de ellas.\n",
    "    \n",
    "    Si encuentra alguno de estos casos, el tokenizador separa la substring considerando estos caracteres y sigue recorriendo el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un posible caso de excepción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excepcion = \"bieniusté\"\n",
    "\n",
    "doc_ex = nlp(excepcion)\n",
    "\n",
    "for token in doc_ex:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cómo agregar reglas que contemplen excepciones:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_case = [{'ORTH': u\"bien\", 'LEMMA': u\"bien\", 'POS': u\"ADJ\"},{'ORTH': 'i','LEMMA':'y','POS':'CCONJ'},\n",
    "               {'ORTH':'usté','LEMMA':'usted','POS':'PRON'}]\n",
    "\n",
    "nlp.tokenizer.add_special_case(u\"bieniusté\", special_case)\n",
    "\n",
    "# vemos la nueva tokenización\n",
    "print([w.text for w in nlp(u\"bieniusté\")]) \n",
    "\n",
    "# vemos la lematización de los tokens\n",
    "print([w.lemma_ for w in nlp(u\"bieniusté\")]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cómo armar un tokenizador propio:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizador que incorporemos a nuestro pipeline debe tomar como input un texto y devolver un objeto Doc que pueda integrarse con el resto de los componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class NLTKTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # Aquí le indicamos al tokenizador qué queremos que haga cuando lo llamamos\n",
    "        words = word_tokenize(text)\n",
    "        # Todos los tokens poseen un espacio que le sigue\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos el tokenizador por defecto por el de NLTK que importamos y vemos qué pasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciamos el tokenizador con un vocabulario\n",
    "nlp_parser.tokenizer = NLTKTokenizer(nlp.vocab)\n",
    "\n",
    "doc_excepcion = nlp_parser(excepcion)\n",
    "\n",
    "ex_tokens = [token.text for token in nlp_parser(excepcion)]\n",
    "ex_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podría ser que nos interesase conocer las palabras que se encuentran alredeor los tokens. El método ```nbor``` nos permite acceder a esta información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 4\n",
    "\n",
    "for token in doc:\n",
    "    if token.text == \"Irene\":\n",
    "        circundantes = [token.nbor(i) for i in range(1,n_words)]\n",
    "        print(token.i, token.text)\n",
    "        print(circundantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Etiquetado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lugo de tokenizar, el pipeline de SpaCy etiquetar y parsea el Doc.Para ello, usa un modelo estadísico para predecir cuál es el tag o etiqueta más adecuado para el contexto.\n",
    "\n",
    "La información que podemos extraer de estas etiquetas es la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Text:** Texto de la palabra original.\n",
    "- **Lemma:** Forma base de la palabra\n",
    "- **POS:** POS tag simple..\n",
    "- **Tag:** POS tag detallado.\n",
    "- **Dep:** Dependencia sintáctica.\n",
    "- **Shape:** Forma de la palabra - puntuación, dígitos, mayúsculas.\n",
    "- **is alpha:** ¿El token es alfabético?\n",
    "- **is stop:** ¿El token es una stopword?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tokens_info = {'text':[],\n",
    "              'lemma':[],\n",
    "              'pos':[],\n",
    "              'tag':[],\n",
    "              'dep':[],\n",
    "              'shape':[],\n",
    "              'is_alpha':[],\n",
    "              'is_stop':[]}\n",
    "    \n",
    "\n",
    "for token in doc:\n",
    "    tokens_info['text'].append(token.text)\n",
    "    tokens_info['lemma'].append(token.lemma_)\n",
    "    tokens_info['pos'].append(token.pos_)\n",
    "    tokens_info['tag'].append(token.tag_)\n",
    "    tokens_info['dep'].append(token.dep_)\n",
    "    tokens_info['shape'].append(token.shape_)\n",
    "    tokens_info['is_alpha'].append(token.is_alpha)\n",
    "    tokens_info['is_stop'].append(token.is_stop)\n",
    "    \n",
    "df = pd.DataFrame(tokens_info)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenemos dudas sobre las etiquetas, podemos obtener un poco más de información sobre ellas de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cómo funciona el sistema de etiquetado:**\n",
    "\n",
    "1. El tokenizador consulta si existen excpciones de tokenización que impliquen que una secuencia de caracteres se mapee con más de un token, y le asigna una etiqueta POS a cada token encontrado.\n",
    "\n",
    "2. Luego, se le asigna a cada token una etiqueta POS extendida con la información morfológica (```tag_```).\n",
    "\n",
    "3. Si algún token no recibió una etiqueta POS en el proceso anterior, se utiliza una tabla de mapeo para estableces sus etiquetas.\n",
    "\n",
    "4. Por último, una regla determinísitca establece cuál es el lema del token en cuestión, considerando tanto la etiqueta POS como la información morfológica encontrada. En el caso del inglés, también se pueden utilizar archivos de excepciones provistos por [WordNet](https://wordnet.princeton.edu/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Links útiles:**\n",
    "\n",
    "[Universal POS tags](https://universaldependencies.org/u/pos/)\n",
    "\n",
    "[Mapeo de tags para español](https://github.com/explosion/spaCy/blob/master/spacy/lang/es/tag_map.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parseo de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent for sent in doc.sents]\n",
    "sentences[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = {\"compact\": True,\"bg\": \"#09a3d5\", \"distances\":1,\n",
    "           \"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
    "\n",
    "displacy.render(sentences[6], style=\"dep\", jupyter=True, options=options)\n",
    "\n",
    "# para ver en otra página\n",
    "# displacy.serve(sentences[6], style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noun chunks**\n",
    "\n",
    "Un noun chunk es una frase nominal base (NP en inglés) que no contiene otra frase nominal incrustada (i.e. no contiene coordinación nominal, frases preposicionales ni cláusulas relativas).\n",
    "\n",
    "SpaCy contiene algunos métodos que nos permiten acceder a la información de estas porciones de texto:\n",
    "\n",
    "- Text: texto original\n",
    "- Root.text: texto original de la palabra que conecta el chunk con el resto de la oracion (núcleo del chunk)\n",
    "- Root.dep_: relación de dependencia que conecta el núcleo del chunk con el núcleo de laoración donde se inserta\n",
    "- Root_head text: núcleo de la oración donde se encuentra el chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[6].text, end='\\n\\n')\n",
    "sent6 = nlp(sentences[6].text)\n",
    "\n",
    "print('{0:30} {1:15} {2:15} {3}'.format('text','root','dep','head'), end='\\n\\n')\n",
    "\n",
    "for chunk in sent6.noun_chunks:\n",
    "    print('{0:30} {1:15} {2:15} {3}'.format(chunk.text, \n",
    "                                            chunk.root.text, \n",
    "                                            chunk.root.dep_, \n",
    "                                            chunk.root.head.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:15} {1:15} {2:15} {3:15} {4}'.format('text','dep','head text','head pos', 'children'), end='\\n\\n')\n",
    "\n",
    "for token in sent6:\n",
    "    print('{0:15} {1:15} {2:15} {3:15} {4}'.format(token.text, \n",
    "                                                   token.dep_, \n",
    "                                                   token.head.text, \n",
    "                                                   token.head.pos_,\n",
    "                                                   [child for child in token.children]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así como podemos ver los nodos hijos de un token, también podemos ver sus  nodos padres o ancestrales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sent6:\n",
    "    print('{0:15} {1}'.format(token.text,\n",
    "                           [ant for ant in token.ancestors]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Links útiles:**\n",
    "\n",
    "[Online demo para visualizar parseos de dependencias](https://explosion.ai/demos/displacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Segmentación de oraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy utiliza el parser de dependencias para determinar los límites de las oraciones en lugar de hacerlo mediante reglas. Esto hace que los límites encontrados dependan del modelo estadístico que se haya entrenado previamente. Sin embargo, también es posible añadir patrones de delimitación o cambiar el segmentador por otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for sent in doc.sents:\n",
    "    print('ORACIÓN',i)\n",
    "    print(sent.text)\n",
    "    print('-----')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si chequeamos el árbol de dependencias, efectivamente vemos que el parser hace depender el título de la primera oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = nlp(sentences[0].text)\n",
    "options={\"distances\":1}\n",
    "displacy.render(sent1, style=\"dep\", options=options, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solucionar esto, SpaCy nos ofrece dos estrategias:\n",
    "\n",
    "- Añadir un componente llamado ```sentencizer``` que separa las oraciones por signos de puntuación tales como el punto (.), el signo de pregunta (?) o el signo de exclamación (!)\n",
    "- Armar nuestro propio segmentador de oraciones basado en reglas y agregarlo al pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para armar un segmentador propio, debemos definir una función que establezca cuál será nuestro separador de oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if re.search('\\n\\n',token.text):\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, agregamos nuestra función al pipeline y volvemos a cargar el documento para que realice la segmentación nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(set_custom_boundaries, name=\"sent_segmenter\", before=\"parser\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "doc = nlp(casa_tomada)\n",
    "sentences_good = list()\n",
    "\n",
    "i = 1\n",
    "for sent in doc.sents:\n",
    "    print('ORACIÓN',i)\n",
    "    print(sent.text)\n",
    "    sentences_good.append(sent.text.strip())\n",
    "    print('-----')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_good[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un NER (Named Entity Recognition) es un sistema que no solo nos permite encontrar o reconocer entidades sino, además, saber de qué tipo son.\n",
    "\n",
    "Ahora bien, ¿qué es una entidad? ¿Cuántos tipos de entidades hay o puede haber? ¿Para qué puede servirnos reconocer entidades?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El NER de SpaCy nos permite reconocer los siguientes atributos de una entidad:\n",
    "\n",
    "- Text: texto original de la entidad\n",
    "- Start: índice de inicio de la entidad en el Doc\n",
    "- End: índice de finalización de la entidad en el Doc\n",
    "- LabeL: etiqueta de la entidad (tipo de entidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences_good[7])\n",
    "sent7 = nlp(sentences_good[7])\n",
    "\n",
    "for ent in sent7.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:50} {1:10} {2:2} {3}'.format('text','start','end','label'),end='\\n\\n')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print('{0:50} {1:5} {2:8} {3}'.format(ent.text, ent.start_char, ent.end_char, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences_good[1:]:\n",
    "    print(sent, end='\\n\\n')\n",
    "    s = nlp(sent)\n",
    "    #print('{0:50} {1:10} {2:2} {3}'.format('text','start','end','label'),end='\\n\\n')\n",
    "    for ent in s.ents:\n",
    "        #print('{0:50} {1:5} {2:8} {3}'.format(ent.text, ent.start_char, ent.end_char, ent.label_))\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    print('-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:50} {1:10} {2:2} {3}'.format('text','start','end','label'),end='\\n\\n')\n",
    "\n",
    "for sent in sentences_good[1:]:\n",
    "    s = nlp(sent)\n",
    "    for ent in s.ents:\n",
    "        print('{0:50} {1:5} {2:8} {3}'.format(ent.text.strip(), ent.start_char, ent.end_char, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualización:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(sent7, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"ORG\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\"}\n",
    "options = {\"ents\": [\"ORG\"], \"colors\": colors}\n",
    "displacy.render(sent7, style=\"ent\", options=options, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo agregar etiquetas sin entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in sent7.ents]\n",
    "print('ANTES', ents)\n",
    "\n",
    "\n",
    "PER = doc.vocab.strings[u\"PER\"]\n",
    "\n",
    "per_ent = [Span(sent7, 0, 1, label=PER),Span(sent7, 13, 15, label=PER)]\n",
    "\n",
    "sent7.ents = per_ent\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in sent7.ents]\n",
    "print('After', ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de accedet al tipo de oración es utilizando el atributo ent_type_ del token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[97])\n",
    "print(doc[97].ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Links útiles:**\n",
    "\n",
    "[Anotaciones del NER de Spacy](https://spacy.io/api/annotation#named-entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.Matcheo mediante reglas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules-base matcher es un componente de SpaCy que posibilita generar reglas con las que contrastar el texto y extraer los fragmentos que se adecúen a ellas. Lo interesante de este componente es que permite combinar expresiones regulares con las anotaciones lingüísticas de los modelos pre-entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "patternSV = [{\"ORTH\": \"Irene\", \"DEP\":'nsubj'},{\"TEXT\":{\"REGEX\":\"\\w+\"},\"OP\":\"*\"},{\"POS\": \"VERB\", \"DEP\": \"ROOT\"},]\n",
    "patternVS = [{\"POS\": \"VERB\", \"DEP\": \"ROOT\"},{\"TEXT\":{\"REGEX\":\"\\w+\"},\"OP\":\"*\"},{\"ORTH\": \"Irene\", \"DEP\":'nsubj'}]\n",
    "patternCOP = [{\"ORTH\": \"Irene\", \"DEP\":'nsubj'},{\"TEXT\":{\"REGEX\":\"\\w+\"},\"OP\":\"*\"},{\"LEMMA\":\"ser\"}]\n",
    "\n",
    "matcher.add(\"IreneSV\", None, patternSV)\n",
    "matcher.add(\"IreneVS\", None, patternVS)\n",
    "matcher.add(\"IreneCOP\", None, patternCOP)\n",
    "\n",
    "matched_sents = list()\n",
    "\n",
    "for sent in sentences_good[1:]:\n",
    "    s = nlp(sent)\n",
    "    matches = matcher(s)\n",
    "    match_ents = list()\n",
    "    if len(matches) > 0:\n",
    "        for match in matches:\n",
    "            match_id, start, end = match\n",
    "            span = s[start:end]\n",
    "            info_match = {\n",
    "                \"start\": span.start_char,\n",
    "                \"end\": span.end_char,\n",
    "                \"label\": \"MATCH\"\n",
    "            }\n",
    "            match_ents.append(info_match)            \n",
    "        matched_sents.append({\"text\": sent, \"ents\": match_ents})\n",
    "\n",
    "        \n",
    "colors = {\"MATCH\": \"linear-gradient(90deg, #b7deed, #49a5bf)\"}\n",
    "options = {\"ents\": [\"MATCH\"], \"colors\": colors}\n",
    "displacy.render(matched_sents, style=\"ent\", options=options, jupyter=True, manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Matcheo mediante reglas](https://explosion.ai/demos/matcher)\n",
    "\n",
    "[Atributos disponibles para tokens](https://spacy.io/api/token#attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
